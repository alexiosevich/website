<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>projects2022</title>
  </head>
  <body link="#0000EE" vlink="#551A8B" text="#000000" bgcolor="#c0c0c0"
    alink="#EE0000">
    <div align="center"><font size="+3" color="#cc0000"><b>Tripods/StemForAll













































          2022 Projects </b></font><br>
      <br>
      <div align="left"><br>
        <b><font size="+1">Derivatives and Neural Networks </font></b><br>
        <br>
        <b>Project supervisors:</b> Alex Iosevich (UR), Azita Mayeli
        (CUNY) and Brian McDonald (UR)<br>
        <br>
        <b>Project description:</b> We are going to study, both
        theoretically and empirically, the impact of adding discrete
        derivatives of the historical data as regressors in order to
        improve the performance of neural network prediction models. In
        simple terms, if we give a neural network a sequence of real
        numbers and ask it to predict the next few values, how helpful
        is it to provide the model with the consecutive difference
        (and/or second differences) of the elements of the sequence? <br>
        <br>
        <b>References: </b>https://www.sciencedirect.com/science/article/abs/pii/S0893608005800206<br>
        <br>
        <b>Team:</b> Amy Fang, Josh Iosevich, Anya Myakushina, Svetlana
        Pack, Maxwell Sun, and Stephanie Wang<br>
        <br>
        <br>
        <font size="+1"><b>Erdos problems and the Vapnik-Chervonenkis
            dimension </b></font><br>
        <br>
        <b>Project supervisors:</b> Alex Iosevich (UR), Brian McDonald
        (UR) and Emmett Wyman (UR) <br>
        <br>
        <b>Project description:</b> We are going to study the existence
        and complexity of finite point configurations in vector spaces
        over finite fields using the notion of VC-dimension, and
        investigate connections with related notions from learning
        theory. <br>
        <br>
        <b>References: </b><a href="https://arxiv.org/abs/2203.03046">arXiv:2203.03046</a>,
        <a href="https://arxiv.org/abs/2108.13231">arXiv:2108.13231</a><span>(www.arxiv.org)










































          <br>
          <br>
          <b>Team: </b></span><span>James Hanby (RIT), Tran Duy Anh Le
          (UR), Maxwell Sun (MIT)<br>
          <br>
          <br>
          <font size="+1"><b>Natural language processing, reinforcement
              learning and web scraping </b></font><br>
          <br>
          <b>Project supervisor:</b> Alex Iosevich (UR) and Scott Kirila
          (Parker Avery)<br>
          <br>
          <b>Project description:</b> We are going to develop a
          mechanism to quickly identify which academic department a
          given university page belongs to, which news outlet a given
          front page story was published in, and similar web scraping
          ideas. In the process we are going to develop a productive
          interaction between reinforcement learning and support vector
          machine methods. <br>
          <br>
          <b>References:</b> https://www.mdpi.com/2078-2489/12/1/38/htm,
https://www.geeksforgeeks.org/top-7-applications-of-natural-language-processing/<br>
          <br>
          <b>Team: </b>Moeed Baradan, Huanyu Chen, Peirong Hao, Yumeng
          He, Bowen Jin, Zhizhi Jing, Junfei Liu, Jiayue Meng, </span><span><span>Yixu












            Qiu, </span>Yukun Yang</span><br>
        <span>&nbsp;<br>
          <br>
          <font size="+1"><b>Neural networks and sales models with
              economic indicators</b></font> <br>
          <br>
          <b>Project supervisor:</b> Alex Iosevich (UR) and Scott Kirila
          (Parker Avery) <br>
          <br>
          <b>Project description:</b> Many sales models starting
          returning less than stellar results during the Covid era, in
          part because the training data came from before the Covid
          period. In this project we are going to take several readily
          available data sets containing sales data and try to come up
          with the right mix of economic (and other) indicators that
          will make predictions as stable as possible across time,
          including the Covid period.<br>
          <br>
          <b>References: </b>arXiv:2105.01036<br>
          <br>
          <b>Team:</b> Moeed Baradan, Veronica Chistaya (house price
          variant), Ji Fang, </span><span><span>Peirong Hao, </span>Bingyi



















          Liu, Kuixian Wu<br>
          <br>
          <br>
          <font size="+1"><b>Neural networks, approximation and
              geometric measure theory </b></font><br>
          <br>
          <b>Project supervisors: </b>Alex Iosevich (UR) and Emmett
          Wyman (UR)<br>
          <br>
          <b>Project description: </b>Neural networks are "universal
          approximators" in that any Lipschitz function can be
          approximated by a neural network arbitrarily closely. This is
          a fundamental result, but many real-life data sets are not
          realistically described by a Lipschitz function because the
          Lipschitz condition limits volatility. In this project we are
          going to explore the universal approximation in the case when
          Lipschitz functions are replaced by more complicated (and
          hopefully more realistic) classes of function, such as
          function with graphs satisfying a suitable fractal dimension
          condition.<br>
          <br>
          <b>References: </b>https://machinelearningmastery.com/neural-networks-are-function-approximators/,









































          http://neuralnetworksanddeeplearning.com/chap4.html<br>
          <br>
          <b>Team:</b> Amy Fang, Zhizhi Jing, Peter MacNeil, Yixu Qiu,
          Rohan Soni, Jake Wellington, Yukun Yang </span><span><br>
          <br>
          <br>
          <font size="+1"><b>Optimal location for charging stations for
              electric cars </b></font><br>
          <br>
          <b>Project supervisors:</b> Alex Iosevich (UR) and Steven
          Senger (Missouri State University) <br>
          <br>
          <b>Project description:</b> We are going to build a model to
          determine the optimal location for charging stations for
          electric cars in Rochester and Ithaca. <br>
          <br>
          <b>References: </b>https://www.sciencedirect.com/science/article/pii/S2352484722001809





























          <br>
          <br>
          <b>Team:</b> Rachel Dennis, Konstantin Dits, Caroline He, Anya
          Myakushina <br>
          <br>
          <br>
          <font size="+1"><b>Modeling seizures using machine learning</b></font>
          <br>
          <br>
          <b>Project supervisor: </b>Alex Iosevich<br>
          <br>
          <b>Project description:</b> It is widely believed in the
          medical community that epileptic seizures do not follow a
          particular daily or weekly time patterns. We believe that the
          techniques of modern data science have not yet been deployed
          in the study of this problem in a systematic way. We are going
          to experiment with a variety of techniques, including
          reinforcement learning, to look for patterns in the
          commercially available data sets. <br>
          <br>
          <b>References: </b>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2739976/,

























          https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5801770/<br>
          <br>
          <b>Team:</b> James Hanby, Marco Minchev, Svetlana Pack <br>
          <br>
          <br>
          <font size="+1"><b>Multi-task learning</b></font> <br>
          <br>
        </span><span><span><b>Project supervisors:</b> Alex Iosevich
            (UR) and Nate Whybra <br>
            <br>
          </span></span><span><span><span><b>Project description:</b></span></span>
          One of the most notable differences between most machine
          learning algorithms and humans is that humans have the ability
          to do multiple tasks. In order to develop AI that can more
          closely mimic human inference and learning capabilities, they
          must be able to generalize information from their environment,
          and use that information flexibly to perform an arbitrary
          number of tasks. We are going to study the idea of building a
          large neural network and training it on multiple tasks, then
          identify substructures of this large network that achieve the
          same efficacy as the large network or neural networks built
          for each task individually. We will identify task-model
          substructures from the large network using binary masks over
          the connections between nodes as well as pruning techniques. <br>
          <br>
        </span><span><span><b>References: </b></span>-Michael Crawshaw.
          “Multi-Task Learning with Deep Neural Networks: A Survey”. In:
          (Sept. 2020). url: <br>
          http://arxiv.org/abs/2009.09796.<br>
          -Shagun Sodhani et al. “Environments and Baseline for
          Multitask Reinforcement Learning”. In: (2021). url: <br>
https://ep2021.europython.eu/media/conference/slides/5sUtdJv-multitask-reinforcement-learning-with-python.pdf<br>
          -Jonathan Frankle and Michael Carbin. “The Lottery Ticket
          Hypothesis: Finding Sparse, Trainable Neural<br>
          Networks”. In: (Mar. 2018).
          url:&nbsp;http://arxiv.org/abs/1803.03635.<br>
          -Hattie Zhou et al. “Deconstructing Lottery Tickets: Zeros,
          Signs, and the Supermask”. In: (May 2019). url:<br>
          http://arxiv.org/abs/1905.01067.<br>
          -Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr.
          “SNIP: Single-shot Network Pruning based<br>
          on Connection Sensitivity”. In: (Oct. 2018).
          url:&nbsp;http://arxiv.org/abs/1810.02340.<br>
          -Network Pruning 101. url:<br>
https://towardsdatascience.com/neural-network-pruning-101-af816aaea61<br>
          <br>
          <b>Team:</b> Ryan Hilton, Bowen Jin, Vicky Wang, Kevin Xu,
          Zhiyao Xu</span><span><br>
        </span><span>
          <meta charset="utf-8">
        </span><span><br>
          <br>
        </span> <span>
          <meta charset="utf-8">
        </span><span><br>
          <br>
          <br>
          <br>
          <br>
          <b> <br>
            <br>
          </b><br>
        </span><span><br>
          <br>
          <br>
          <br>
          <br>
          <br>
        </span> </div>
      <br>
    </div>
  </body>
</html>
